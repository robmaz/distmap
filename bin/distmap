#!/usr/bin/env perl -w
use strict;
use warnings;
use Getopt::Long;
use Pod::Usage;
use File::Which;
use File::Temp;

use FindBin qw/$RealBin/;
use lib "$RealBin/../lib/perl5/site_perl";
use Utility;
use GenomeIndex;
use DataProcess;
use DataUpload;
use HadoopTrimming;
use HadoopMapping;
use DataDownload;
use DataMerge;
use DataDownloadAndMerge;
use DownloadTrimmedRead;
use DataCleanup;
use ReadToolsUtils;

my $script = "distmap";

my $version = "3.1.0-alpha";

my $usage = qq{
$script: Run NGS data mapping on hadoop cluster

Usage: perl $script

--hadoop-home			Give the full path of hadoop folder. MANDATORY parameter.
				hadoop home path should be identical in master, secondary namenode and all slaves.
				Example: --hadoop-home /usr/local/hadoop

--reference-fasta      		Reference fasta file full path. MANDATORY parameter.

--reference-index-archive	Reference index archive created by previous DistMap run.
				This will avoid the re-indexing of same verion of reference fasta.
				Example: --reference-index-archive /home/test_output/distmap_mapping1/refarch.tgz

--input                		provide input fasta files. Either as a pair or single Fastq file.
				For Paired-end data --input 'read1.fastq,read2.fastq'
				For Single-end data --input 'read.fastq'
				Multiple inputs are possible to repeat --input parameter

				--input 'sample1_1.fastq,sample1_2.fastq' --input 'sample2_1.fastq,sample2_2.fastq'
				This is important to give single or a pair input file within
				single or double quote and if paired file, it must be comma
				seperated. MANDATORY parameter.

				For Exonerate input will be a FASTA file format file

--output              		Full path of output folder where final output will be kept. MANDATORY parameter.

--only-index    		Step1: This option will create genome index and create archieve to upload on HDFS. OPTIONAL parameter

--only-process    		Step2: This option will 1) convert FASTQ files into 1 line format, 2) create genome index
			        and 3) create archieve to send for job and exit. OPTIONAL parameter

--only-hdfs-upload          	Step3: This option assume that data processing is done and only upload reads and archieve created in
				data process step will be loaded into HDFS file system and exit. OPTIONAL parameter

--only-trim             OBSOLETE: this option does nothing, trimming is now performed while uploading into the HDFS

--no-trim            		DEPRECATED: This option will skip the trimming (done on upload to the HDFS).

--trim-script-path		OBSOLETE: trimming will be performed by ReadTools while uploading into the HDFS

--trim-args			Give the trimming arguments (ReadTools syntax) EXAMPLE: --trim-args "--trimmer MottQualityTrimmer --mottQualityThreshold 20 --readFilter ReadLengthReadFilter --minReadLength 50 --disable5pTrim"

--only-map            		Step5: This option will assume that data is already loaded into HDFS and will only
				run map on cluster

--only-hdfs-download          	Step4: This option assume that mapping already done on cluster and files will be downloaded
				from cluster to local output directory. OPTIONAL parameter

--only-merge          		Step6: This option assume that data already in local directory from HDFS
				and now will be merged and create a single SAM or BAM output file. OPTIONAL parameter

--only-delete-temp          	Step7: This is the last step of piepline. This option will delete the
				mapping data from HDFS file system as well as from local temp directory.
				OPTIONAL parameter

--mapper              		Mapper name [bwa,tophat,gsnap,bowtie,soap]. MANDATORY parameter.
				Example: --mapper bwa

--mapper-path         		Mapper executable full path. Mapper executables should be in the
				same location on all nodes (slaves). MANDATORY parameter.
				Example: --mapper-path /usr/local/hadoop/bwa

--gsnap-output-split		GSNAP has a feature to split different type of maaping output in different
                                SAM files.

				For detail do gsnap --help and you can see --split--output.
				  --split-output=STRING   Basename for multiple-file output, separately for nomapping,
                                   halfmapping_uniq, halfmapping_mult, unpaired_uniq, unpaired_mult,
                                   paired_uniq, paired_mult, concordant_uniq, and concordant_mult results (up to 9 files,
                                   or 10 if --fails-as-input is selected, or 3 for single-end reads)

--picard		Full path to either picard.jar or the picard launcher script. MANDATORY unless the launcher
            script can be found in the path.
					Example: --picard /usr/local/bin/picard

--picard-mergesamfiles-jar     OBSOLETE: picard is distributed in a single file now. Use --picard-jar instead.

--picard-sortsam-jar     	OBSOLETE: picard is distributed in a single file now. Use --picard-jar instead.

--readtools               Full path to either ReadTools.jar or the readtools launcher script.
                          MANDATORY unless the launcher script can be found in the path.
        Example: --readtools /usr/local/bin/readtools

--tmp-dir           Change the default folder for local temporary files. OPTIONAL,
                    defaults to the output folder.
				Example: --tmp-dir /Volumes/Temp2

--mapper-args        		Arguments for mapping:
				BWA mapping for aln command:
					Example --mapper-args "-o 1 -n 0.01 -l 200 -e 12 -d 12"
					Note: Define BWA parameters correctly accoring to the version is used here.
				TopHat:
					Example: --mapper-args "--mate-inner-dist 200 --max-multihits 40 --phred64-quals"
					Note: Define TopHat parameters correctly accoring to the version is used here.
				GSNAP mapping:
					Example: --mapper-args "--pairexpect 200 --quality-protocol illumina"
					Note: Define gsnap parameters correctly accoring to the version is used here.
					For detail about parameters visit [http://research-pub.gene.com/gmap/]
				bowtie mapping:
					Example: --mapper-args "--sam"
					Note: Define gsnap parameters correctly accoring to the version is used here.
					For detail about parameters visit [http://bowtie-bio.sourceforge.net/index.shtml]
				SOAPAlinger:
					Example: --mapper-args "-m 400 -x 600"
					Note: Define SOAPaligner parameters correctly accoring to the version is used here.
					For detail about parameters visit [http://soap.genomics.org.cn/soapaligner.html]
Please note that processor parameters not required in arguments. Example in case of BWA mapping dont give -t parameter.
This parameter is given by DistMap internally.

--bwa-sampe-args      	Arguments for BWA sampe or samse module.
			bwa sampe for paired-end reads
			bwa samse for single-end reads
			Example --bwa-sampe-args "-a 500 -s"

--output-format    		Output file format either SAM or BAM.
				Default: BAM

--job-desc    		Give a job description which will be dispalyed in JobTracker webpage.
				Default: <mapper name> mapping.
Hadoop streaming Parameters:

--queue-name    		If your hadoop has Capacity Scheduler then provide --queue-name.
				Example: --queue-name pg1

--verbose                 	To print inputs on screen.
--help                	To run a test for all dependency and all.

};




### This number need to define. How many processors Hadoop can use on each nodes.
my $number_of_processors = 3; #18;


my $reference_fasta="";
my $refindex_archive="";
my @input_files = ();
my $output_directory="";

my $only_index=0;
my $only_process=0;
my $only_trim=0;
my $no_trim=0;
my $only_map=0;
my $only_hdfs_upload=0;
my $only_hdfs_download =0;
my $only_merging=0;
my $only_delete_temp=0;
my $only_download_reads=0;

my $gsnap_output_split=0;
#my $hadoop_home=$ENV{"HADOOP_HOME"};
my $hadoop_home=$ENV{"HADOOP_HOME"};

my $mapper="";
my $mapper_path="";
my $mapper_args="";
my @mapper=();
my @mapper_path=();
my @mapper_args=();

my $bwa_sampe_args="";
my $picard=`which picard`;
my $picard_mergesamfiles_jar=""; # OBSOLETE
my $picard_sortsam_jar=""; # OBSOLETE
my $picard_mark_duplicates_jar="";
my $readtools_arg = `which readtools`; # TODO: should this be called after parsing if not provided? I think that it is more consistent.
my $tmp_dir = "";
my $output_format="bam";
my $job_desc="";
my $queue_name="pg1";
my $job_priority="VERY_HIGH";
my $hadoop_scheduler="Fair";

my $mappers_exe_archive = "";

my $trim_script_path="";
my $trim_args=""; # TODO: standard readtools trimming args

my $verbose = 0;
my $test = 0;
my $help = 0;



#### Trimming parameters

    my $qualThreshold=20;
    my $minLength=40;
    my $fastqtype="illumina";
    my $discardRemainingNs=0;
    my $trimQuality=0; ## TODO: be careful with this argument (it is really no-trim-quality switch)
    my $no5ptrim=0;
    my $nozip=0;


GetOptions(
	"reference-fasta=s"			=>\$reference_fasta,	# to check
	"input=s"                           	=> \@input_files,      # to check
	"output=s"                          	=> \$output_directory,	# to check
	"only-index" 				=> \$only_index,
	"only-process" 				=> \$only_process,
	"only-hdfs-upload" 			=> \$only_hdfs_upload,
	"only-trim" 				=> \$only_trim,
	"no-trim" 				=> \$no_trim,
	"trim-script-path=s"			=> \$trim_script_path,
	"trim-args"				=> \$trim_args,
	"only-map" 				=> \$only_map,
	"only-hdfs-download" 			=> \$only_hdfs_download,
	"only-merge" 				=> \$only_merging,
	"only-delete-temp" 			=> \$only_delete_temp,
	"only-download-trimmed-reads"		=> \$only_download_reads,
	"reference-index-archive=s"         	=> \$refindex_archive,
	"quality-threshold=i"   		=>\$qualThreshold,
	"min-length=i"          		=>\$minLength,
	"fastq-type=s"          		=>\$fastqtype,
	"discard-internal-N"    		=>\$discardRemainingNs,
	"no-trim-quality"       		=>$trimQuality,
	"no-5p-trim"            		=>\$no5ptrim,
	"disable-zipped-output" 		=>\$nozip,
	"hadoop-home=s" 			=>\$hadoop_home,
	"mappers-exe-archive=s" 		=>\$mappers_exe_archive,


	"mapper=s" 				=> \@mapper,		# to check
	"mapper-path=s"				=> \@mapper_path,
	"picard=s"				=> \$picard, # to check
	"picard-mergesamfiles-jar=s"		=> \$picard_mergesamfiles_jar,	# OBSOLETE
	"picard-sortsam-jar=s"			=> \$picard_sortsam_jar,	# OBSOLETE
	"readtools=s"         => \$readtools_arg,
	"tmp-dir=s"           => \$tmp_dir,
	"mapper-args=s"				=> \@mapper_args,
	"bwa-sampe-args=s"			=> \$bwa_sampe_args,
	"output-format=s"			=> \$output_format,
	"job-desc=s"				=> \$job_desc,	# to check
	"queue-name=s"				=> \$queue_name,	# to check
	"gsnap-output-split"			=> \$gsnap_output_split,	# to check
	"verbose"        		        => \$verbose,
	"help"          		        =>\$help,
	"h"          		        	=>\$help
) or pod2usage(-msg=>"Wrong options",-verbose=>1);

pod2usage(-verbose=>2) if $help;

if ( defined $hadoop_home and $hadoop_home ne "" ) {
    # if $hadoop_home is set, make sure there is no trailing slash
    $hadoop_home =~ s/\/$//;
}
else {
    # try to find "hadoop" in path and prune "/bin/hadoop" from the full path
    $hadoop_home = `which hadoop`;
    $hadoop_home =~ s/\/bin\/hadoop$//;
}

if ( $hadoop_home eq "" ) {
	pod2usage(-msg=>"\n\tERROR: cannot define HADOOP_HOME. Use --hadoop-home or add 'hadoop' to your PATH");
}

# error if ReadTools is not found 
if ( $readtools_arg eq "" ) {
	pod2usage(-msg=>"\n\tERROR: no ReadTools found. Use --readtools or add a wrapper/jar to your PATH");
}

## get the readtools command to be run
my $readtools = ReadToolsUtils::get_readtools_runnable_cmd($readtools_arg);
if ( `$readtools --version` lt ReadToolsUtils::VERSION ) {
	pod2usage(-msg=>"\n\tERROR: Unsupported ReadTools (\"$readtools_arg\") - required >= " . ReadToolsUtils::VERSION );
}

if ( $picard =~ /\.jar$/ ) {
  $readtools = "eval java \\\$JAVA_OPTS -jar $picard";
}

unless ( $tmp_dir eq "" or
  ( -d $tmp_dir and -w $tmp_dir and -x $tmp_dir ) ) {
  	pod2usage(-msg=>"\n\tERROR: \"$tmp_dir\" is not a usable temporary folder (permissions?)");
}


if($verbose) {
	print STDERR "Following inputs are provided to run $script:\n";
	print STDERR "  reference genome fasta:\t$reference_fasta\n";
	print STDERR "  input fastq files:\t@input_files\n";
	print STDERR "  local output directory:\t$output_directory\n";
	print STDERR "  only fastq file process:\t$only_process\n";
	print STDERR "  only hdfs upload:\t$only_hdfs_upload\n";
	print STDERR "  (DEPRECATED) only trim reads:\t$only_trim\n";
	print STDERR "  only mapping reads:\t$only_map\n";
	print STDERR "  only hdfs download:\t$only_hdfs_download\n";
	print STDERR "  only merging the hadoop mapping output files :\t$only_merging\n";
	print STDERR "  only delete temperory files and folders:\t$only_delete_temp\n";
	print STDERR "  hadoop home:\t$hadoop_home\n";
	print STDERR "  mapper name:\t@mapper\n";
	print STDERR "  mapper path:\t@mapper_path\n";
	print STDERR "  picard:\t$picard\n";
	print STDERR "  (OBSOLETE) picard MergeSamFiles.jar:\t$picard_mergesamfiles_jar\n";
	print STDERR "  (OBSOLETE) picard SortSam.jar:\t$picard_sortsam_jar\n";
	print STDERR "  readtools:\t$readtools_arg ($readtools)\n";
	print STDERR "  tmpdir:\t$tmp_dir\n";
	print STDERR "  mapper arguments:\t@mapper_args\n";
	print STDERR "  bwa sampe arguments:\t$bwa_sampe_args\n";
	print STDERR "  output format:\t$output_format\n";
	print STDERR "  job description:\t$job_desc\n";
	exit(0);
}

# respect trim_args if given on the command line
## TODO: fix - the trim_args might be provided in the old perl format - is that still expected?
if ($trim_args eq "") {
	# TODO: this should be removed if all the parameters are removed
	$trim_args = ReadToolsUtils::get_trimming_args($qualThreshold, $minLength, $discardRemainingNs, $trimQuality, $no5ptrim);
}

###

my $args_dict = {};


$args_dict->{"only_index"} = $only_index;
$args_dict->{"only_process"} = $only_process;
$args_dict->{"only_hdfs_upload"} = $only_hdfs_upload;
$args_dict->{"only_trim"} = $only_trim;
$args_dict->{"only_map"} = $only_map;
$args_dict->{"only_hdfs_download"} = $only_hdfs_download;
$args_dict->{"only_merging"} = $only_merging;
$args_dict->{"only_download_reads"} = $only_download_reads;
$args_dict->{"only_delete_temp"} = $only_delete_temp;
$args_dict->{"refindex_archive"} = $refindex_archive;
$args_dict->{"mappers_exe_archive"} = $mappers_exe_archive;
$args_dict->{"hadoop_home"} = $hadoop_home;
$args_dict->{"output_directory"} = $output_directory;
$args_dict->{"exec_arch"} = "DistMap_Mapper_Archive.tgz";
$args_dict->{"extracted_execarch"} = "execarch";

$args_dict->{"ref_arch"} = "DistMap_Genome_Index_Archive.tgz";
$args_dict->{"extracted_refarch"} = "Genome_Index_Archive";
$args_dict->{"mapper_path"} = \@mapper_path;
$args_dict->{"reference_fasta"} = $reference_fasta;

$args_dict->{"picard"} = $picard;

# if ($picard_mergesamfiles_jar ne "" and $picard_sortsam_jar ne "") {
# 	$args_dict->{"picard_mergesamfiles_jar"} = $picard_mergesamfiles_jar;
# 	$args_dict->{"picard_sortsam_jar"} = $picard_sortsam_jar;
# }
# else {
# 	$args_dict->{"picard_mergesamfiles_jar"} = "$picard_jar MergeSamFiles";
# 	$args_dict->{"picard_sortsam_jar"} = "$picard_jar SortSam";
# 	$args_dict->{"picard_mark_duplicates_jar"} = "$picard_jar MarkDuplicates";
# }

$args_dict->{"readtools"} = $readtools;
$args_dict->{"tmp_dir"} = $tmp_dir;

my $step_hash = {};
$step_hash = Utility::get_steps($args_dict);

if (scalar(keys %$step_hash)==1 and $args_dict->{"only_index"}) {
	#print "only_index------\n";
	Utility::create_dir($args_dict);
	if (scalar(keys %$step_hash)>0) {
		foreach my $step ( sort { $a <=> $b } keys %$step_hash) {
			#print "$step $step_hash->{$step}\n";
			$args_dict->{"upload_index"}=1;
			$step_hash->{$step}->start($args_dict);
		}
	}

}
elsif (scalar(keys %$step_hash)==1 and $args_dict->{"only_delete_temp"}) {
	#print "only_delete_temp------\n";
	Utility::create_dir($args_dict);
	### Get HADOOP executables and Streaming jar
	Utility::check_hadoop($args_dict);

	if (scalar(keys %$step_hash)>0) {
		foreach my $step ( sort { $a <=> $b } keys %$step_hash) {
			#print "$step $step_hash->{$step}\n";

			$step_hash->{$step}->start($args_dict);
		}
	}

}
else {

    $args_dict = check_inputs();

    if ( $args_dict->{"trimming_flag"} ) {
        #
    }

    Utility::check_genome_index($args_dict);

#### checking the mappers integrity. If multiple mappers are using in a single command . Implemented on 12-04-2016

    if ( scalar( keys %$step_hash ) > 0 ) {
        if ( ( $args_dict->{"only_index"} ) or ( $args_dict->{"only_map"} ) ) {
            check_mapper_integritiy($args_dict);
        }
    }
    else {
        check_mapper_integritiy($args_dict);
    }

    if ( scalar( keys %$step_hash ) > 0 ) {
        foreach my $step ( sort { $a <=> $b } keys %$step_hash ) {
            $step_hash->{$step}->start($args_dict);
        }
    }
    else {
        Utility::run_whole_pipeline($args_dict);
    }
}
exit;


sub check_mapper_integritiy {
	my ($args_dict) = @_;

	$args_dict->{"mapper"} = \@mapper;
	$args_dict->{"mapper_path"} = \@mapper_path;
	$args_dict->{"mapper_args"} = \@mapper_args;

	if (scalar(@{$args_dict->{"mapper_path"}})<1) {
		print "\n\tERROR: 3 arguments (--mapper --mapper-path --mapper-args) must be in the same order\n\n";
		print "\n\tExample: \"--mapper bowtie2 --mapper novoalign --mapper bwa --mapper-args \"<bowtie2 mapper parameters>\" --mapper-args \"<novoalign mapper parameters>\" --mapper-args \"<bwa mapper parameters>\" --mapper-path <bowtie2 mapper path> --mapper-path <novoalign mapper path> --mapper-path <bwa mapper path>\"\n\n";

		print qq(\n\tExample: \"--mapper bowtie2 --mapper novoalign --mapper bwa --mapper-args \"--phred33 --end-to-end -X 1500\" --mapper-args \"-i 250,75 -F STDFQ -o SAM -r RANDOM\" --mapper-args \"bwasw\" --mapper-path $ENV{"DISTMAP_HOME"}/executables/bowtie2-2.2.6/bowtie2 --mapper-path $ENV{"DISTMAP_HOME"}/executables/novocraft/novoalign --mapper-path $ENV{"DISTMAP_HOME"}/executables/bwa-0.7.13/bwa\"\n\n);

		exit(1);

	}
	elsif (scalar(@{$args_dict->{"mapper"}})==scalar(@{$args_dict->{"mapper_path"}}) and scalar(@{$args_dict->{"mapper_path"}})==scalar(@{$args_dict->{"mapper_args"}})) {
		#print "mappers are correct\n";
	}

	else {
		print "\n\tERROR: 3 arguments (--mapper --mapper-path --mapper-args) must be in the same order\n\n";
		print "\n\tExample: \"--mapper bowtie2 --mapper novoalign --mapper bwa --mapper-args \"<bowtie2 mapper parameters>\" --mapper-args \"<novoalign mapper parameters>\" --mapper-args \"<bwa mapper parameters>\" --mapper-path <bowtie2 mapper path> --mapper-path <novoalign mapper path> --mapper-path <bwa mapper path>\"\n\n";

		print qq(\n\tExample: \"--mapper bowtie2 --mapper novoalign --mapper bwa --mapper-args \"--phred33 --end-to-end -X 1500\" --mapper-args \"-i 250,75 -F STDFQ -o SAM -r RANDOM\" --mapper-args \"bwasw\" --mapper-path $ENV{"DISTMAP_HOME"}/executables/bowtie2-2.2.6/bowtie2 --mapper-path $ENV{"DISTMAP_HOME"}/executables/novocraft/novoalign --mapper-path $ENV{"DISTMAP_HOME"}/executables/bwa-0.7.13/bwa\"\n\n);

		exit(1);
	}



}




sub check_inputs {

	my $args_dict = {};

	if ($output_directory =~ /\/$/g) {
		$output_directory =~ s/\/$//g;
	}

	$args_dict->{"only_download_reads"} = $only_download_reads;


	$args_dict->{"number_of_processors"} = $number_of_processors;
	#$args_dict->{"usage"} = $usage;
	$args_dict->{"reference_fasta"} = $reference_fasta;
	$args_dict->{"input_files"} = \@input_files;
	$args_dict->{"output_directory"} = $output_directory;
	$args_dict->{"refindex_archive"} = $refindex_archive;

	$args_dict->{"mappers_exe_archive"} = $mappers_exe_archive;

	$args_dict->{"only_index"} = $only_index;
	$args_dict->{"only_process"} = $only_process;
	$args_dict->{"only_trim"} = $only_trim;
	$args_dict->{"no_trim"} = $no_trim;
	$args_dict->{"only_download_reads"} = $only_download_reads;
	$args_dict->{"trim_script_path"} = $trim_script_path;
	$args_dict->{"trim_args"} = $trim_args;

	$args_dict->{"nozip"} = $nozip; # this is a flag to output trimmed fastq files as zipped file.

	$args_dict->{"only_map"} = $only_map;
	$args_dict->{"only_hdfs_upload"} = $only_hdfs_upload;
	$args_dict->{"only_hdfs_download"} = $only_hdfs_download;
	$args_dict->{"only_merging"} = $only_merging;
	$args_dict->{"only_delete_temp"} = $only_delete_temp;
	$args_dict->{"hadoop_home"} = $hadoop_home;
	$args_dict->{"mapper"} = \@mapper;
	$args_dict->{"mapper_path"} = \@mapper_path;

	$args_dict->{"picard"} = $picard;

	$args_dict->{"readtools"} = $readtools;
	$args_dict->{"tmp_dir"} = $tmp_dir;

	$mapper_args =~ s/\"//g;
	$args_dict->{"mapper_args"} = \@mapper_args;
	$args_dict->{"bwa_sampe_args"} = $bwa_sampe_args;
	$args_dict->{"output_format"} = $output_format;
	$args_dict->{"job_desc"} = $job_desc;
	$args_dict->{"gsnap_output_split"} = $gsnap_output_split;

	if (!$queue_name) {
		$queue_name="default";
	}

	if (!$job_priority) {
		$job_priority="NORMAL";
	}

	$job_priority = uc($job_priority);

	$args_dict->{"queue_name"} = "mapreduce.job.queuename=$queue_name";

	$args_dict->{"job_priority"} = "mapreduce.job.priority=$job_priority";

	if ($args_dict->{"mapper"}->[0] =~ /gsnap/i ) {
		$args_dict->{"block_size"} = "128m";
	}
	else {
		$args_dict->{"block_size"} = "32m";
	}
	$args_dict->{"exec_arch"} = "DistMap_Mapper_Archive.tgz";
	$args_dict->{"extracted_execarch"} = "execarch";

	$args_dict->{"ref_arch"} = "Genome_Index_Archive.tgz";
	$args_dict->{"extracted_refarch"} = "Genome_Index_Archive";

  if ( $args_dict->{"no_trim"} ) {
    $args_dict->{"trimming_flag"} = "";
  }
  else {
    $args_dict->{"trimming_flag"} = $trim_args; # TODO: trimming_flag will be passed to readtools as is
  }

	### Get HADOOP executables and Streaming jar
	Utility::check_hadoop($args_dict);

	### Create all temp folder and directory
	Utility::create_dir($args_dict);

	#get username
	my $username = getlogin();
	my $uname = (getpwuid($<))[0];
	my @groups = split'\s',$(; # $( means real group id (gid), list (seperated by spaces) of groups. $) means effective group id (gied), list (seperated by spaces) of groups
	my @group_name = ();
	foreach (@groups) {
		push(@group_name,getgrgid($_));
	}
	#print "$username,,$uname,,,@group_name\n";
	$args_dict->{"username"} = $uname;
	$args_dict->{"groupname"} = "hadoop";
	#$args_dict->{'username'}:$args_dict->{'groupname'}
	#exit();
	return $args_dict;

}


=head1 NAME

DistMap_v1.2/distmap - This pipeline maps NGS reads on a local hadoop cluster.

=head1 SYNOPSIS

 DistMap_v1.2/distmap --reference-fasta /reads/dmel_genome.fasta --input "/reads/5M_reads_1.fastq,/reads/5M_reads_2.fastq" --mapper bwa --mapper-path /executables/bwa --picard-mergesamfiles-jar /executables/MergeSamFiles.jar --picard-sortsam-jar /executables/SortSam.jar --mapper-args "-l 200" --output-format sam --hadoop-scheduler Fair --queue-name pg1 --job-priority VERY_HIGH --output /reads/bwa/bwa_mapping --job-desc "BWA mapping 5M 1"


=head1 DistMap OPTIONS

=over 4

=item B<--reference-fasta>

 Reference fasta file full path. MANDATORY parameter.

=item B<--input>

 provide input fasta files. Either as a pair or single Fastq file.
 For Paired-end data --input 'read1.fastq,read2.fastq'
 For Single-end data --input 'read.fastq'
 Multiple inputs are possible to repeat --input parameter

 --input 'sample1_1.fastq,sample1_2.fastq' --input 'sample2_1.fastq,sample2_2.fastq'
 This is important to give single or a pair input file within
 single or double quote and if paired file, it must be comma
 seperated. MANDATORY parameter.

=item B<--output>

 Full path of output folder where final output will be kept. MANDATORY parameter.

=item B<--only-index>
 Step1: This option will create genome index and create archieve to upload on HDFS. OPTIONAL parameter


=item B<--only-process>

 Step2: This option will 1) convert FASTQ files into 1 line format, 2) create genome index
 and 3) create archieve to send for job and exit. OPTIONAL parameter

=item B<--only-hdfs-upload>

 Step3 This option assume that data processing is done and only upload reads and archieve created in
 data process step will be loaded into HDFS file system and exit. OPTIONAL parameter

=item B<--only-map>

 Step4: This option will assume that data is already loaded into HDFS and will only
 run map on cluster

=item B<--only-hdfs-download>

 Step5: This option assume that mapping already done on cluster and files will be downloaded
 from cluster to local output directory. OPTIONAL parameter

=item B<--only-merge>

 Step6: This option assume that data already in local directory from HDFS
 and now will be merged and create a single SAM or BAM output file. OPTIONAL parameter

=item B<--only-delete-temp>

 Step7: This is the last step of piepline. This option will delete the
 mapping data from HDFS file system as well as from local temp directory.
 OPTIONAL parameter

=item B<--hadoop-home>

 Give the full path of hadoop folder. MANDATORY parameter.
 hadoop home path should be identical in master, secondary namenode and all slaves.
 Example: --hadoop-home /usr/local/hadoop

=item B<--mapper>

 Mapper name [bwa,tophat,gsnap,bowtie,soap]. MANDATORY parameter.
 Example: --mapper bwa

=item B<--mapper-path>
 Mapper executable full path. MANDATORY parameter.
 Example: --mapper-path /usr/local/hadoop/bwa

=item B<--gsnap-output-split>

 GSNAP has a feature to split different type of maaping output in different SAM files.

	or detail do gsnap --help and you can see --split--output.
	--split-output=STRING   Basename for multiple-file output, separately for nomapping,
        halfmapping_uniq, halfmapping_mult, unpaired_uniq, unpaired_mult,
        paired_uniq, paired_mult, concordant_uniq, and concordant_mult results (up to 9 files,
        or 10 if --fails-as-input is selected, or 3 for single-end reads)

=item B<--picard>

 Full path to either picard.jar or the picard lajuncher script. MANDATORY unless picard is in the path.
 Example: --picard /usr/local/bin/picard

=item B<--picard-mergesamfiles-jar>

OBSOLETE: picard is distributed in a single file now. Use --picard instead.

=item B<--picard-sortsam-jar>

 OBSOLETE: picard is distributed in a single file now. Use --picard instead.

=item B<--readtools>

 ReadTools path, either to the jar or to the launcher script. MANDATORY unless
 the launcher script is in the path.
 Example: --readtools /usr/local/share/java/ReadTools.jar

=item B<--tmp-dir>

 Path to a folder to use for temporary files. A temporary folder with a unique name
 will be created in this folder. OPTIONAL, you may want to set this if the default
 location (the output folder) does not provide enough space for sorting while merging
 with ReadTools.
 Example: --tmp-dir /Volumes/Temp2

=item B<--mapper-args>

 Arguments for mapping:
	BWA mapping for aln command:
		Example --mapper-args "-o 1 -n 0.01 -l 200 -e 12 -d 12"
		Note: Define BWA parameters correctly accoring to the version is used here.

	TopHat mapping:
		Example: --mapper-args "--mate-inner-dist 200 --max-multihits 40 --phred64-quals"
		Note: Define TopHat parameters correctly accoring to the version is used here.

	GSNAP mapping:
		Example: --mapper-args "--pairexpect 200 --quality-protocol illumina"
		Note: Define gsnap parameters correctly accoring to the version is used here.
		For detail about parameters visit [http://research-pub.gene.com/gmap/]

	bowtie mapping:
		Example: --mapper-args "--sam"
		Note: Define gsnap parameters correctly accoring to the version is used here.
		For detail about parameters visit [http://bowtie-bio.sourceforge.net/index.shtml]

	SOAPAlinger:
		Example: --mapper-args "-m 400 -x 600"
		Note: Define SOAPaligner parameters correctly accoring to the version is used here.
		For detail about parameters visit [http://soap.genomics.org.cn/soapaligner.html]

 Please note that processor parameters not required in arguments. Example in case of BWA mapping dont give -t parameter.
 This parameter is given by DistMap internally.

=item B<--bwa-sampe-args>

 Arguments for BWA sampe or samse module.
	bwa sampe for paired-end reads
	bwa samse for single-end reads
	Example --bwa-sampe-args "-a 500 -s"

=item B<--output-format>

 Output file format either SAM or BAM.
 Default: BAM

=item B<--job-desc>

 Give a job description which will be dispalyed in JobTracker webpage.
 Default: <mapper name> mapping.


=item B<--queue-name>

 If your hadoop has Capacity Scheduler then provide --queue-name.
 Example: --queue-name pg1




=item B<--only-trim>

OBSOLETE: This option does nothing. Trimming is now done on upload to the HDFS.


=item B<--no-trim>

DEPRECATED: This option will skip the trimming.
This argument will be removed in favor of --trim-args.


=item B<--trim-script-path>

OBSOLETE: trimming is now always done with ReadTools.


=item B<--trim-args>

Give the trimming arguments (ReadTools syntax) EXAMPLE: --trim-args "--trimmer MottQualityTrimmer --mottQualityThreshold 20 --readFilter ReadLengthReadFilter --minReadLength 50 --disable5pTrim"


=item B<--verbose>

To print inputs on screen.

=item B<--help>

To run a test for all dependency and all.


=head1 TRIMMING OPTIONS

=over 4

=item B<--quality-threshold>

DEPRECATED: minimum average quality; A modified Mott algorithm is used for trimming; the threshold is used for calculating a score: score = quality_at_base - threshold; default=20
This argument will be removed in favor of --trim-args.

=item B<--fastq-type>

The encoding of the quality characters; Must either be 'sanger' or 'illumina';

 Using the notation suggested by Cock et al (2009) the following applies:
 'sanger'   = fastq-sanger: phred encoding; offset of 33
 'solexa'   = fastq-solexa: -> NOT SUPPORTED
 'illumina' = fastq-illumina: phred encoding: offset of 64

 See also:
 Cock et al (2009) The Sanger FASTQ file format for sequecnes with quality socres,
 and the Solexa/Illumina FASTQ variants;

default=illumina

=item B<--discard-internal-N>

flag, if set reads having internal Ns will be discarded; default=off

=item B<--min-length>

DEPRECATED: The minimum length of the read after trimming; default=40
This argument will be removed in favor of --trim-args.


=item B<--no-trim-quality>

DEPRECATED: toggle switch: switch of trimming of quality.
This argument will be removed in favor of --trim-args.

=item B<--no-5p-trim>

DEPRECATED: toggle switch; Disable 5'-trimming (quality and 'N'); May be useful for the identification of duplicates when using trimming of reads.
Duplicates are usually identified by the 5' mapping position which should thus not be modified by trimming. default=off
This argument will be removed in favor of --trim-args.

=item B<--disable-zipped-output>

DEPRECATED: This option is ignored.


=back

=head1 Details

 DistMap is an integrated pipeline to map FASTQ formatted short reads on a local Hadoop cluster framework.
 It takes all input from local disk and return single SAM or BAM mapping file in local disk. User does not need to upload or download data in Hadoop manually.


=head1 AUTHORS

 Ram Vinay Pandey
 Christian Schloetterer

=cut
