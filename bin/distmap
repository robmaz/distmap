#!/usr/bin/env perl
use strict;
use warnings;
use Getopt::Long;
use Pod::Usage;
use File::Which;
use File::Temp;

use FindBin qw/$RealBin/;
use lib "$RealBin/../lib/perl5/site_perl";
use Utility;
use GenomeIndex;
use DataProcess;
use DataUpload;
use HadoopMapping;
use DataDownload;
use DataMerge;
use DataDownloadAndMerge;
use DownloadTrimmedRead;
use DataCleanup;
use ReadToolsUtils;

# current version
use constant VERSION => "3.1.0-alpha";

## TODO - make a constant!!
## TODO - should be exposed? or maybe depends on other variable (e.g., priority?)
### This number need to define. How many processors Hadoop can use on each nodes
my $number_of_processors = 3; #18;



# ARGUMENT VARIABLES
## REQUIRED
my $reference_fasta="";
my $output_directory="";
my @input_files = ();
my $mapper_args="";
my @mapper=();
my @mapper_path=();
my @mapper_args=();

## PSEUDO-REQUIRED
my $hadoop_home=$ENV{"HADOOP_HOME"};
my $picard=`which picard`;
my $readtools_arg = `which readtools`;

## OPTIONAL
### pipeline
my $only_index=0;
my $only_process=0;
my $only_hdfs_upload=0;
my $only_map=0;
my $only_hdfs_download =0;
my $only_delete_temp=0;
my $trim_args="";
### others
my $tmp_dir = "";
my $output_format="bam";
my $refindex_archive="";
my $queue_name="pg1";
my $job_desc="";
my $gsnap_output_split=0; # TODO - make conditional
my $bwa_sampe_args=""; # TODO - make conditional

my $help = 0;
my $version = 0;

## DEPRECATED (log warning if used?) - TODO: remove
my $no_trim=0;
my $only_merging=0;
my $qualThreshold=20;
my $discardRemainingNs=0;
my $minLength=40;
my $trimQuality=0; ## TODO: be careful with this argument (it is really no-trim-quality switch)
my $no5ptrim=0;
my $verbose = 0;

## OBSOLETE (log warning if used?) - TODO: remove
my $picard_mergesamfiles_jar="";
my $picard_sortsam_jar="";
my $picard_mark_duplicates_jar=""; # TODO: is not even used - remove
my $only_trim=0;
my $trim_script_path="";
my $fastqtype="illumina";
my $nozip=0;

## UNDOCUMENTED - TODO: documment
my $only_download_reads=0;
my $mappers_exe_archive = "";

## TODO - supposedly options but not in the GetOptions block!!! cannot be specified
## TODO - add to the parsing or remove support for them and default always!
## TODO - they appear in the example!!
my $job_priority="VERY_HIGH";

## UNUSED variable!! - TODO: remove?
my $test = 0;

## parse command line
GetOptions(
	# REQUIRED (to check)
	"reference-fasta=s"				=>\$reference_fasta,
	"output=s"						=> \$output_directory,
	"input=s"						=> \@input_files,
	"mapper=s"						=> \@mapper,
	"mapper-path=s"					=> \@mapper_path,
	"mapper-args=s"					=> \@mapper_args,

	# PSEUDO-REQUIRED (to check in PATH)
	"hadoop-home=s"					=>\$hadoop_home,
	"picard=s"						=> \$picard,
	"readtools=s"					=> \$readtools_arg,

	# OPTIONAL
	## pipeline
	"only-index"					=> \$only_index,
	"only-process"					=> \$only_process, # TODO - process/upload still different? (otherwise, deprecate)
	"only-hdfs-upload"				=> \$only_hdfs_upload,
	"only-map"						=> \$only_map,
	"only-hdfs-download"			=> \$only_hdfs_download,
	"only-delete-temp"				=> \$only_delete_temp,
	"trim-args"						=> \$trim_args,
	## other
	"tmp-dir=s"						=> \$tmp_dir,
	"output-format=s"				=> \$output_format,
	"reference-index-archive=s"		=> \$refindex_archive,
	"queue-name=s"					=> \$queue_name,
	"job-desc=s"					=> \$job_desc,
	"gsnap-output-split"			=> \$gsnap_output_split, # TODO - this should be a conditional argument on GSNAP mapper (throw otherwise)
	"bwa-sampe-args=s"				=> \$bwa_sampe_args, # TODO - this should be a conditional argument on BWA-ALN mapper (throw otherwise)
	"help|h"						=>\$help,
	"version|v"						=>\$version,

	# DEPRECATED (log warning if used?) - TODO: remove support
	## triming
	"no-trim"						=> \$no_trim,
	"only-merge"					=> \$only_merging,
	"quality-threshold=i"			=>\$qualThreshold,
	"discard-internal-N"			=>\$discardRemainingNs,
	"min-length=i"					=>\$minLength,
	"no-trim-quality"				=>$trimQuality,
	"no-5p-trim"					=>\$no5ptrim,
	## others
	"verbose"						=> \$verbose,

	# OBSOLETE (log warning if used?) - TODO: remove
	"picard-mergesamfiles-jar=s"	=> \$picard_mergesamfiles_jar,
	"picard-sortsam-jar=s"			=> \$picard_sortsam_jar,
	"only-trim"						=> \$only_trim,
	"trim-script-path=s"			=> \$trim_script_path,
	"fastq-type=s"					=>\$fastqtype,
	"disable-zipped-output"			=>\$nozip,
	
	
	## TODO: undocummented options!!!
	"only-download-trimmed-reads"	=> \$only_download_reads,
	"mappers-exe-archive=s"			=>\$mappers_exe_archive

) or pod2usage(-msg=>"Wrong options",-verbose=>1);

# if --version is specified, print it and exit
if ($version) {
	print VERSION, "\n";
	exit(0)
}
# print the help if requrested and exit
pod2usage(-verbose=>2, -exitval=>0) if $help;


## ARGUMENTS CHECK
### TODO: check the required arguments:
### TODO: - reference fasta exists (and other requirements for it; e.g., dictionary or index)
### TODO: - output directory can be created (and other requirements for it; e.g., empty)
### TODO: - inputs exist and are correctly formatted (and other requirements for it; e.g., even number for pair-end)
### TODO: - mapper, mapper-path and mapper-args are correct (e.g., mapper and mapper-path pointing to same, mapper-args can be found in mapper help)


## check pseudo-required arguments
### HADOOP_HOME
if ( defined $hadoop_home and $hadoop_home ne "" ) {
    # if $hadoop_home is set, make sure there is no trailing slash
    $hadoop_home =~ s/\/$//;
}
else {
    # try to find "hadoop" in path and prune "/bin/hadoop" from the full path
    $hadoop_home = `which hadoop`;
    $hadoop_home =~ s/\/bin\/hadoop$//;
}
if ( $hadoop_home eq "" ) {
	pod2usage(-msg=>"\n\tERROR: cannot define HADOOP_HOME. Use --hadoop-home or add 'hadoop' to your PATH");
}

<<<<<<< HEAD
### READTOOLS
#### error if ReadTools is not found 
=======
# error if ReadTools is not found
>>>>>>> faaf392ad21dfeed05ba5eb85fc46b9dcf246e9b
if ( $readtools_arg eq "" ) {
	pod2usage(-msg=>"\n\tERROR: no ReadTools found. Use --readtools or add a wrapper/jar to your PATH");
}
#### get the readtools command to be run
my $readtools = ReadToolsUtils::get_readtools_runnable_cmd($readtools_arg);
if ( `$readtools --version` lt ReadToolsUtils::VERSION ) {
	pod2usage(-msg=>"\n\tERROR: Unsupported ReadTools (\"$readtools_arg\") - required >= " . ReadToolsUtils::VERSION );
}

### PICARD
if ( $picard =~ /\.jar$/ ) {
  $picard = "eval java \\\$JAVA_OPTS -jar $picard";
}

unless ( $tmp_dir eq "" or
  ( -d $tmp_dir and -w $tmp_dir and -x $tmp_dir ) ) {
  	pod2usage(-msg=>"\n\tERROR: \"$tmp_dir\" is not a usable temporary folder (permissions?)");
}

### TODO: check optional arguments (e.g., tmp-dir exists or should be created)
### TODO: log a warning for deprecated options and/or obsolete ones


## print if verbose is found
if($verbose) {
	print STDERR "WARNING: this list of inputs might be out-of-date (--verbose is deprecated)";
	print STDERR "Following inputs are provided to run $0:\n";
	print STDERR "  reference genome fasta:\t$reference_fasta\n";
	print STDERR "  input fastq files:\t@input_files\n";
	print STDERR "  local output directory:\t$output_directory\n";
	print STDERR "  only fastq file process:\t$only_process\n";
	print STDERR "  only hdfs upload:\t$only_hdfs_upload\n";
	print STDERR "  (DEPRECATED) only trim reads:\t$only_trim\n";
	print STDERR "  only mapping reads:\t$only_map\n";
	print STDERR "  only hdfs download:\t$only_hdfs_download\n";
	print STDERR "  (DEPRECATED) only merging the hadoop mapping output files :\t$only_merging\n";
	print STDERR "  only delete temperory files and folders:\t$only_delete_temp\n";
	print STDERR "  hadoop home:\t$hadoop_home\n";
	print STDERR "  mapper name:\t@mapper\n";
	print STDERR "  mapper path:\t@mapper_path\n";
	print STDERR "  picard:\t$picard\n";
	print STDERR "  readtools:\t$readtools_arg ($readtools)\n";
	print STDERR "  tmpdir:\t$tmp_dir\n";
	print STDERR "  mapper arguments:\t@mapper_args\n";
	print STDERR "  bwa sampe arguments:\t$bwa_sampe_args\n";
	print STDERR "  output format:\t$output_format\n";
	print STDERR "  job description:\t$job_desc\n";
	exit(0);
}

# respect trim_args if given on the command line
if ($trim_args eq "") {
	# TODO: this should be removed if all the parameters are removed
	$trim_args = ReadToolsUtils::get_trimming_args($qualThreshold, $minLength, $discardRemainingNs, $trimQuality, $no5ptrim);
}

## TODO: this args_dict should be the one provided ot the GetOptions
## TODO: this could be a hash %args_hash and then the variables should still be available
## TODO: this will simplify the code and make it less error prone
## TODO: the simplification would be something like (just some param examples):
## my %h = ("reference-fasta" => "", 'output-format' => "bam", "only-trim" => 0);
## GetOptions (\%h, 'eference-fasta=s', "output-format=s", "only-trim") or pod2usage(-msg=>"Wrong options",-verbose=>1);
my $args_dict = {};


$args_dict->{"only_index"} = $only_index;
$args_dict->{"only_process"} = $only_process;
$args_dict->{"only_hdfs_upload"} = $only_hdfs_upload;
$args_dict->{"only_trim"} = $only_trim;
$args_dict->{"only_map"} = $only_map;
$args_dict->{"only_hdfs_download"} = $only_hdfs_download;
$args_dict->{"only_merging"} = $only_merging;
$args_dict->{"only_download_reads"} = $only_download_reads;
$args_dict->{"only_delete_temp"} = $only_delete_temp;
$args_dict->{"refindex_archive"} = $refindex_archive;
$args_dict->{"mappers_exe_archive"} = $mappers_exe_archive;
$args_dict->{"hadoop_home"} = $hadoop_home;
$args_dict->{"output_directory"} = $output_directory;
$args_dict->{"exec_arch"} = "DistMap_Mapper_Archive.tgz";
$args_dict->{"extracted_execarch"} = "execarch";

$args_dict->{"ref_arch"} = "DistMap_Genome_Index_Archive.tgz";
$args_dict->{"extracted_refarch"} = "Genome_Index_Archive";
$args_dict->{"mapper_path"} = \@mapper_path;
$args_dict->{"reference_fasta"} = $reference_fasta;

$args_dict->{"picard"} = $picard;

$args_dict->{"readtools"} = $readtools;
$args_dict->{"tmp_dir"} = $tmp_dir;

my $step_hash = {};
$step_hash = Utility::get_steps($args_dict);

if (scalar(keys %$step_hash)==1 and $args_dict->{"only_index"}) {
	#print "only_index------\n";
	Utility::create_dir($args_dict);
	if (scalar(keys %$step_hash)>0) {
		foreach my $step ( sort { $a <=> $b } keys %$step_hash) {
			#print "$step $step_hash->{$step}\n";
			$args_dict->{"upload_index"}=1;
			$step_hash->{$step}->start($args_dict);
		}
	}

}
elsif (scalar(keys %$step_hash)==1 and $args_dict->{"only_delete_temp"}) {
	#print "only_delete_temp------\n";
	Utility::create_dir($args_dict);
	### Get HADOOP executables and Streaming jar
	Utility::check_hadoop($args_dict);

	if (scalar(keys %$step_hash)>0) {
		foreach my $step ( sort { $a <=> $b } keys %$step_hash) {
			#print "$step $step_hash->{$step}\n";

			$step_hash->{$step}->start($args_dict);
		}
	}

}
else {

    $args_dict = check_inputs();

    if ( $args_dict->{"trimming_flag"} ) {
        #
    }

    Utility::check_genome_index($args_dict);

#### checking the mappers integrity. If multiple mappers are using in a single command . Implemented on 12-04-2016

    if ( scalar( keys %$step_hash ) > 0 ) {
        if ( ( $args_dict->{"only_index"} ) or ( $args_dict->{"only_map"} ) ) {
            check_mapper_integritiy($args_dict);
        }
    }
    else {
        check_mapper_integritiy($args_dict);
    }

    if ( scalar( keys %$step_hash ) > 0 ) {
        foreach my $step ( sort { $a <=> $b } keys %$step_hash ) {
            $step_hash->{$step}->start($args_dict);
        }
    }
    else {
        Utility::run_whole_pipeline($args_dict);
    }
}
exit;


sub check_mapper_integritiy {
	my ($args_dict) = @_;

	$args_dict->{"mapper"} = \@mapper;
	$args_dict->{"mapper_path"} = \@mapper_path;
	$args_dict->{"mapper_args"} = \@mapper_args;

	if (scalar(@{$args_dict->{"mapper_path"}})<1) {
		print "\n\tERROR: 3 arguments (--mapper --mapper-path --mapper-args) must be in the same order\n\n";
		print "\n\tExample: \"--mapper bowtie2 --mapper novoalign --mapper bwa --mapper-args \"<bowtie2 mapper parameters>\" --mapper-args \"<novoalign mapper parameters>\" --mapper-args \"<bwa mapper parameters>\" --mapper-path <bowtie2 mapper path> --mapper-path <novoalign mapper path> --mapper-path <bwa mapper path>\"\n\n";

		print qq(\n\tExample: \"--mapper bowtie2 --mapper novoalign --mapper bwa --mapper-args \"--phred33 --end-to-end -X 1500\" --mapper-args \"-i 250,75 -F STDFQ -o SAM -r RANDOM\" --mapper-args \"bwasw\" --mapper-path $ENV{"DISTMAP_HOME"}/executables/bowtie2-2.2.6/bowtie2 --mapper-path $ENV{"DISTMAP_HOME"}/executables/novocraft/novoalign --mapper-path $ENV{"DISTMAP_HOME"}/executables/bwa-0.7.13/bwa\"\n\n);

		exit(1);

	}
	elsif (scalar(@{$args_dict->{"mapper"}})==scalar(@{$args_dict->{"mapper_path"}}) and scalar(@{$args_dict->{"mapper_path"}})==scalar(@{$args_dict->{"mapper_args"}})) {
		#print "mappers are correct\n";
	}

	else {
		print "\n\tERROR: 3 arguments (--mapper --mapper-path --mapper-args) must be in the same order\n\n";
		print "\n\tExample: \"--mapper bowtie2 --mapper novoalign --mapper bwa --mapper-args \"<bowtie2 mapper parameters>\" --mapper-args \"<novoalign mapper parameters>\" --mapper-args \"<bwa mapper parameters>\" --mapper-path <bowtie2 mapper path> --mapper-path <novoalign mapper path> --mapper-path <bwa mapper path>\"\n\n";

		print qq(\n\tExample: \"--mapper bowtie2 --mapper novoalign --mapper bwa --mapper-args \"--phred33 --end-to-end -X 1500\" --mapper-args \"-i 250,75 -F STDFQ -o SAM -r RANDOM\" --mapper-args \"bwasw\" --mapper-path $ENV{"DISTMAP_HOME"}/executables/bowtie2-2.2.6/bowtie2 --mapper-path $ENV{"DISTMAP_HOME"}/executables/novocraft/novoalign --mapper-path $ENV{"DISTMAP_HOME"}/executables/bwa-0.7.13/bwa\"\n\n);

		exit(1);
	}



}



## TODO: is this really necessary??
## TODO: the inputs should be populated before, and here there is no real "check"
## TODO: in addition, this is overriding stuff that shouldn't be overriden
sub check_inputs {

	my $args_dict = {};

	if ($output_directory =~ /\/$/g) {
		$output_directory =~ s/\/$//g;
	}

	$args_dict->{"only_download_reads"} = $only_download_reads;


	$args_dict->{"number_of_processors"} = $number_of_processors;
	$args_dict->{"reference_fasta"} = $reference_fasta;
	$args_dict->{"input_files"} = \@input_files;
	$args_dict->{"output_directory"} = $output_directory;
	$args_dict->{"refindex_archive"} = $refindex_archive;

	$args_dict->{"mappers_exe_archive"} = $mappers_exe_archive;

	$args_dict->{"only_index"} = $only_index;
	$args_dict->{"only_process"} = $only_process;
	$args_dict->{"only_trim"} = $only_trim;
	$args_dict->{"no_trim"} = $no_trim;
	$args_dict->{"only_download_reads"} = $only_download_reads;
	$args_dict->{"trim_script_path"} = $trim_script_path;
	$args_dict->{"trim_args"} = $trim_args;

	$args_dict->{"nozip"} = $nozip; # this is a flag to output trimmed fastq files as zipped file.

	$args_dict->{"only_map"} = $only_map;
	$args_dict->{"only_hdfs_upload"} = $only_hdfs_upload;
	$args_dict->{"only_hdfs_download"} = $only_hdfs_download;
	$args_dict->{"only_merging"} = $only_merging;
	$args_dict->{"only_delete_temp"} = $only_delete_temp;
	$args_dict->{"hadoop_home"} = $hadoop_home;
	$args_dict->{"mapper"} = \@mapper;
	$args_dict->{"mapper_path"} = \@mapper_path;

	$args_dict->{"picard"} = $picard;

	$args_dict->{"readtools"} = $readtools;
	$args_dict->{"tmp_dir"} = $tmp_dir;

	$args_dict->{"mapper_args"} = \@mapper_args;
	$args_dict->{"bwa_sampe_args"} = $bwa_sampe_args;
	$args_dict->{"output_format"} = $output_format;
	$args_dict->{"job_desc"} = $job_desc;
	$args_dict->{"gsnap_output_split"} = $gsnap_output_split;

	if (!$queue_name) {
		$queue_name="default";
	}

	if (!$job_priority) {
		$job_priority="NORMAL";
	}

	$job_priority = uc($job_priority);

	$args_dict->{"queue_name"} = "mapreduce.job.queuename=$queue_name";

	$args_dict->{"job_priority"} = "mapreduce.job.priority=$job_priority";

	if ($args_dict->{"mapper"}->[0] =~ /gsnap/i ) {
		$args_dict->{"block_size"} = "128m";
	}
	else {
		$args_dict->{"block_size"} = "32m";
	}
	$args_dict->{"exec_arch"} = "DistMap_Mapper_Archive.tgz";
	$args_dict->{"extracted_execarch"} = "execarch";

	$args_dict->{"ref_arch"} = "Genome_Index_Archive.tgz";
	$args_dict->{"extracted_refarch"} = "Genome_Index_Archive";

  if ( $args_dict->{"no_trim"} ) {
    $args_dict->{"trimming_flag"} = "";
  }
  else {
    $args_dict->{"trimming_flag"} = $trim_args; # TODO: trimming_flag will be passed to readtools as is
  }

	### Get HADOOP executables and Streaming jar
	Utility::check_hadoop($args_dict);

	### Create all temp folder and directory
	Utility::create_dir($args_dict);

	#get username
	my $username = getlogin();
	my $uname = (getpwuid($<))[0];
	my @groups = split'\s',$(; # $( means real group id (gid), list (seperated by spaces) of groups. $) means effective group id (gied), list (seperated by spaces) of groups
	my @group_name = ();
	foreach (@groups) {
		push(@group_name,getgrgid($_));
	}
	#print "$username,,$uname,,,@group_name\n";
	$args_dict->{"username"} = $uname;
	$args_dict->{"groupname"} = "hadoop";
	#$args_dict->{'username'}:$args_dict->{'groupname'}
	#exit();
	return $args_dict;

}


# here the file ends and the documentation starts
__END__

=head1 NAME

distmap - wrapper around different NGS mappers for distributed computation using Hadoop



=head1 SYNOPSIS

B<distmap> [--help | -h] [--version | -v]
--reference-fasta I<FILE> --output I<PATH> --input "I<INPUT>"
--mapper I<MAPPER> --mapper-path I<FILE> --mapper-args "I<ARGS>"
[--hadoop-home I<PATH>] [--readtools I<FILE>] [--picard I<FILE>] [I<OPTIONS>...]



=head1 DESCRIPTION

DistMap is an integrated pipeline to map FASTQ formatted short reads on a local
Hadoop cluster framework. It takes all input from local disk and return single
SAM or BAM mapping file in local disk. User does not need to upload or download
data in Hadoop manually.



=head1 OPTIONS


=head2 MANDATORY OPTIONS

=over 4

=item B<--reference-fasta> I<FILE>

 Reference fasta file full path.

=item B<--output> I<PATH>

 Full path of output folder where final output will be kept.

=item B<--input> "I<INPUT>"

 Quoted (single or double quote) input fasta file(s), either as a single or
 paired FASTQ file. For paired file, it must be comma separated. Examples:

 	For paired-end data B<--input> I<'read1.fastq,read2.fastq'>
 	For single-end data B<--input> I<'read.fastq'>
 	Multiple inputs are possible by repeating B<--input> (e.g.,
 	B<--input> I<'sample1_1.fastq,sample1_2.fastq'> B<--input> I<'sample2_1.fastq,sample2_2.fastq'>)

=item B<--mapper> I<FILE>

 Mapper name [bwa,tophat,gsnap,bowtie,soap].

=item B<--mapper-path> I<FILE>
 Mapper executable full path.

=item B<--mapper-args> "I<ARGS>"

 Arguments for mapping:
	BWA mapping for aln command:
		Example --mapper-args "-o 1 -n 0.01 -l 200 -e 12 -d 12"
		Note: Define BWA parameters correctly accoring to the version is used here.

	TopHat mapping:
		Example: --mapper-args "--mate-inner-dist 200 --max-multihits 40 --phred64-quals"
		Note: Define TopHat parameters correctly accoring to the version is used here.

	GSNAP mapping:
		Example: --mapper-args "--pairexpect 200 --quality-protocol illumina"
		Note: Define gsnap parameters correctly accoring to the version is used here.
		For detail about parameters visit [http://research-pub.gene.com/gmap/]

	bowtie mapping:
		Example: --mapper-args "--sam"
		Note: Define gsnap parameters correctly accoring to the version is used here.
		For detail about parameters visit [http://bowtie-bio.sourceforge.net/index.shtml]

	SOAPAlinger:
		Example: --mapper-args "-m 400 -x 600"
		Note: Define SOAPaligner parameters correctly accoring to the version is used here.
		For detail about parameters visit [http://soap.genomics.org.cn/soapaligner.html]

 Please note that processor parameters not required in arguments.
 Example in case of BWA mapping dont give -t parameter.
 This parameter is given by DistMap internally.

=back


=head2 PSEUDO-MANDATORY OPTIONS

=over 4

=item B<--hadoop-home> I<PATH>

 Give the full path of hadoop folder. Required unless environmental variable HADOOP_HOME
 is present.
 The Hadoop home should be identical in master, secondary namenode and all slaves.

=item B<--readtools> I<PATH>

 Full path to either ReadTools.jar or the readtools launcher script. Required unless launcher script
 is in the PATH.

=item B<--picard> I<PATH>

 Full path to either picard.jar or the picard launcher script. Required unless launcher script
 is in the PATH.

=back


=head2 PIPELINE OPTIONS

=over 4

=item B<--only-index>
 Step1: This option will create genome index and create archieve to upload on HDFS.

=item B<--only-process>

 Step2: This option will 1) convert FASTQ files into 1 line format, 2) create genome index
 and 3) create archieve to send for job and exit.

=item B<--only-hdfs-upload>

 Step3 This option assume that data processing is done and only upload reads and archieve created in
 data process step will be loaded into HDFS file system and exit.

=item B<--only-map>

 Step4: This option will assume that data is already loaded into HDFS and will only
 run map on cluster.

=item B<--only-hdfs-download>

Step5: This option assume that mapping already done on cluster and files will be downloaded
and merged from the cluster to the local output directory using ReadTools, generating a
sorted SAM/BAM file (see B<--output-format>)

=item B<--only-delete-temp>

 Step6: This is the last step of piepline. This option will delete the
 mapping data from HDFS file system as well as from local temp directory.

=item B<--trim-args> "I<ARGS>"

Give the trimming arguments (ReadTools syntax).
EXAMPLE:
	B<--trim-args> "I<--trimmer MottQualityTrimmer --mottQualityThreshold 20 --readFilter ReadLengthReadFilter --minReadLength 50 --disable5pTrim>"


=back


=head2 OPTIONAL OPTIONS

=over 4

=item B<--tmp-dir> I<PATH>

 Path to a folder to use for temporary files. A temporary folder with a unique name
 will be created in this folder. OPTIONAL, you may want to set this if the default
 location (the output folder) does not provide enough space for sorting while merging
 with ReadTools.
 Example: --tmp-dir /Volumes/Temp2

=item B<--output-format> I<FORMAT>

 Output file format either SAM or BAM.
 Default: BAM

=item B<--reference-index-archive> I<FILE>

Path to the genome index archive (I<Genome_Index_Archive.tgz>, generated with previous
runs or the B<--only-index> option). Providing the file avoids re-indexing the reference
with the same mapper.

B<IMPORTANT:> the archive should be generated with the same version of the mapper
provided by B<--mapper-path>.

EXAMPLE:
 	B<--reference-index-archive> I</home/DistMap_output/Genome_Index_Archive.tgz>

Note: Please make sure that the genome Index was created with the SAME VERSION of mapper(s) which you are using for mapping. Different versions of mappers will result in problems.

=item B<--job-desc> "I<DESC>"

 Give a job description which will be dispalyed in JobTracker webpage.
 Default: <mapper name> mapping.

=item B<--queue-name> I<QUEUE>

 If your hadoop has Capacity Scheduler then provide --queue-name.
 Example: --queue-name pg1

=item B<--gsnap-output-split>

 GSNAP has a feature to split different type of mapping output in different SAM files.

	or detail do gsnap --help and you can see --split--output.
	--split-output=STRING   Basename for multiple-file output, separately for nomapping,
        halfmapping_uniq, halfmapping_mult, unpaired_uniq, unpaired_mult,
        paired_uniq, paired_mult, concordant_uniq, and concordant_mult results (up to 9 files,
        or 10 if --fails-as-input is selected, or 3 for single-end reads)

=item B<--bwa-sampe-args> "I<ARGS>"

 Arguments for BWA sampe or samse module.
	bwa sampe for paired-end reads
	bwa samse for single-end reads
	Example --bwa-sampe-args "-a 500 -s"

=item B<--help>, B<-h>

 Print help and exit.

=item B<--version>, B<-v>

Print version and exit.

=back


=head2 DEPRECATED OPTIONS

Arguments still in use, but that might disappear in the following version.

=over 4

=item B<--only-merge>

This option would launch Step5 in the same way as B<--only-hdfs-download>.
Next version will remove this option in favor of B<--only-hdfs-download>

=item B<--no-trim>

 This option will skip the trimming. Next version will decide if trimming
 will be performed by using B<--trim-args>

=item B<--verbose>

 To print inputs on screen and exit. Next version will have a B<--dry-run>
 argument for showing the pipeline to be performed and arguments.

=item B<--quality-threshold> I<INT>

 Minimum average quality for Mott's trimming algorithm (default=20).
 Next version will use B<--trim-args> to provide this trimming feature.

=item B<--discard-internal-N>

 Flag, if set reads having internal Ns will be discarded while trimming.
 Next version will use B<--trim-args> to provide this triming feature.

=item B<--min-length> I<INT>

The minimum length of the read after trimming (default=40).
Next version will use B<--trim-args> to provide this trimming feature.

=item B<--no-trim-quality>

 Toggle switch: switch of trimming of quality.
 Next version will use B<--trim-args> to provide this trimming feature.

=item B<--no-5p-trim>

 Toggle switch; Disable 5'-trimming (quality and 'N').
 Next version will use B<--trim-args> to provide this trimming feature.

=back


=head2 OBSOLETE OPTIONS

Unused arguments maintained for previous version compatibility.

=over 4

=item B<--picard-mergesamfiles-jar> I<PATH>

This option does nothing. picard is distributed in a single file now. Use B<--picard> instead.

=item B<--picard-sortsam-jar> I<PATH>

This option does nothing. picard is distributed in a single file now. Use B<--picard> instead.

=item B<--only-trim>

This option does nothing. Trimming is now done on upload to the HDFS.

=item B<--trim-script-path> I<FILE>

Trimming is now always done with ReadTools.

=item B<--fastq-type> I<TYPE>

This option does nothing. Quality encoding is directly detected by ReadTools.

=item B<--disable-zipped-output>

This option does nothing.

=back



=head1 EXAMPLES

=over 6


=item Mandatory parameters

distmap --reference-fasta /reads/dmel_genome.fasta --output /reads/bwa/bwa_mapping
--input "/reads/5M_reads_1.fastq,/reads/5M_reads_2.fastq"
--mapper bwa --mapper-path /usr/local/hadoop/bwa --mapper-args "-l 200"

=item If pseudo-mandatory options are not in the PATH

distmap --reference-fasta /reads/dmel_genome.fasta --output /reads/bwa/bwa_mapping
--input "/reads/5M_reads_1.fastq,/reads/5M_reads_2.fastq"
--mapper bwa --mapper-path /usr/local/hadoop/bwa --mapper-args "-l 200"
--hadoop-home /usr/local/hadoop
--readtools /usr/local/share/java/ReadTools.jar
--picard /usr/local/share/java/picard.jar

=item Hadoop-related job management

distmap --reference-fasta /reads/dmel_genome.fasta --output /reads/bwa/bwa_mapping
--input "/reads/5M_reads_1.fastq,/reads/5M_reads_2.fastq"
--mapper bwa --mapper-path /usr/local/hadoop/bwa --mapper-args "-l 200"
--queue-name pg1 --job-priority VERY_HIGH --job-desc "BWA mapping 5M 1"

=item Trimming when uploading to HDFS

distmap --reference-fasta /reads/dmel_genome.fasta --output /reads/bwa/bwa_mapping
--input "/reads/5M_reads_1.fastq,/reads/5M_reads_2.fastq"
--mapper bwa --mapper-path /usr/local/hadoop/bwa --mapper-args "-l 200"
--trim-args "--trimmer MottQualityTrimmer --mottQualityThreshold 20
--readFilter ReadLengthReadFilter --minReadLength 50 --disable5pTrim"

=back



=head1 AUTHORS

The original code for distmap was written by Ram Vinay Pandey and
Christian Schloetterer as described in the citation.

Version 3 is a major improvement of the code developed by Rupert
Mazzucco and Daniel Gomez-Sanchez



=head1 CITATION

Please, cite Pandey RV, Schloetterer C (2013) DistMap: A Toolkit for
Distributed Short Read Mapping on a Hadoop Cluster.
PLOS ONE 8(8): e72614. https://doi.org/10.1371/journal.pone.0072614



=cut
